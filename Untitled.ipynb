{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06d08b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13676/2766408000.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# read the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m---> 54\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# read the file\n",
    "with open('text_file.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# sentence splitting\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "# word tokenization\n",
    "words = word_tokenize(text.lower())\n",
    "num_words = len(words)\n",
    "\n",
    "# stopwords removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.casefold() not in stop_words]\n",
    "num_filtered_words = len(filtered_words)\n",
    "\n",
    "# stemming\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "# unique words\n",
    "unique_words = list(set(filtered_words))\n",
    "num_unique_words = len(unique_words)\n",
    "\n",
    "# type-token ratio\n",
    "ttr = num_unique_words / num_filtered_words\n",
    "\n",
    "# word length distribution\n",
    "word_lengths = [len(word) for word in filtered_words]\n",
    "word_lengths_count = Counter(word_lengths)\n",
    "\n",
    "# sentence length distribution\n",
    "sentence_lengths = [len(sent) for sent in sentences]\n",
    "sentence_lengths_count = Counter(sentence_lengths)\n",
    "\n",
    "# parts of speech distribution\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "pos_tags_count = Counter(pos_tags)\n",
    "\n",
    "# named entity recognition\n",
    "ner_tags = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "ner_tags_count = Counter(ner_tags)\n",
    "\n",
    "# average word length\n",
    "avg_word_length = sum(word_lengths) / num_filtered_words\n",
    "\n",
    "# average syllables per word\n",
    "syllable_counts = [TextBlob(word).syllables for word in filtered_words]\n",
    "avg_syllables_per_word = sum(syllable_counts) / num_filtered_words\n",
    "\n",
    "# punctuation distribution\n",
    "punctuation = [char for char in text if char in '.,?!;:-()\"\\'']\n",
    "punctuation_count = Counter(punctuation)\n",
    "\n",
    "# capitalization distribution\n",
    "capitalization = [char.isupper() for char in text if char.isalpha()]\n",
    "capitalization_count = Counter(capitalization)\n",
    "\n",
    "# sentiment analysis\n",
    "blob = TextBlob(text)\n",
    "polarity = blob.sentiment.polarity\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "# print the results\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "print(f\"Number of words: {num_words}\")\n",
    "print(f\"Number of filtered words: {num_filtered_words}\")\n",
    "print(f\"Number of unique words: {num_unique_words}\")\n",
    "print(f\"Type-token ratio: {ttr:.2f}\")\n",
    "print(f\"Word length distribution: {word_lengths_count}\")\n",
    "print(f\"Sentence length distribution: {sentence_lengths_count}\")\n",
    "print(f\"Parts of speech distribution: {pos_tags_count}\")\n",
    "print(f\"Named entity recognition: {ner_tags_count}\")\n",
    "print(f\"Average word length: {avg_word_length:.2f}\")\n",
    "print(f\"Average syllables per word: {avg_syllables_per_word:.2f}\")\n",
    "print(f\"Punctuation distribution: {punctuation_count}\")\n",
    "print(f\"Capitalization distribution: {capitalization_count}\")\n",
    "print(f\"Sentiment polarity: {polarity:.2f}\")\n",
    "print(f\"Sentiment subjectivity: {subjectivity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa604583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.5.1-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adity\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp39-cp39-win_amd64.whl (482 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.6-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.3)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: typing-extensions, colorama, catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 colorama-0.4.6 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.6 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c68a480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\adity\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\adity\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adity\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\adity\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9963999",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13676/1553972540.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adity\\AppData\\Local\\Temp/ipykernel_13676/1553972540.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m spacy download en_core_web_sm\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774d20c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1888/2464209263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[1;32m---> 54\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from collections import Counter\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tree import Tree\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def get_word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def get_sentence_count(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def get_syllable_count(word):\n",
    "    d = cmudict.dict()\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def get_total_syllable_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    syllable_count = 0\n",
    "    for word in words:\n",
    "        syllable_count += get_syllable_count(word)\n",
    "    return syllable_count\n",
    "\n",
    "def get_avg_sentence_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(words) / len(sentences)\n",
    "\n",
    "def get_avg_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    syllable_count = 0\n",
    "    for word in words:\n",
    "        syllable_count += get_syllable_count(word)\n",
    "    return syllable_count / len(words)\n",
    "\n",
    "def get_ttr(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def get_flesch_kincaid_grade_level(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    syllable_count = get_total_syllable_count(text)\n",
    "    return 0.39 * (len(words) / len(sentences)) + 11.8 * (syllable_count / len(words)) - 15.59\n",
    "\n",
    "def get_word_length_distribution(text):\n",
    "    words = word_tokenize(text)\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    return FreqDist(word_lengths)\n",
    "\n",
    "def get_sentence_length_distribution(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    return FreqDist(sentence_lengths)\n",
    "\n",
    "def get_pos_distribution(text):\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
    "    return pos_counts\n",
    "\n",
    "def get_named_entity_recognition(text):\n",
    "    named_entities = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sentence))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                named_entities.append(' '.join(c[0] for c in chunk))\n",
    "    return named_entities\n",
    "\n",
    "def get_punctuation_distribution(text):\n",
    "    words = word_tokenize(text)\n",
    "    punctuation_counts = Counter(word for word in words if word in string.punctuation)\n",
    "    return punctuation_counts\n",
    "\n",
    "def get_capitalization_distribution(text):\n",
    "    words = word_tokenize(text)\n",
    "    capitalization_counts = Counter(word for word in words if word[0].isupper())\n",
    "    return capitalization_counts\n",
    "\n",
    "def get_unique_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words))\n",
    "\n",
    "def get_word_stems(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    return stems\n",
    "\n",
    "def get_word_frequencies(text):\n",
    "    words = word_tokenize(text)\n",
    "    return FreqDist(words)\n",
    "\n",
    "def get_collocation_frequencies(text):\n",
    "    words = word_tokenize(text)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    return finder.nbest(bigram_measures.raw_freq, 10)\n",
    "\n",
    "def get_ngram_frequencies(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    ngrams = nltk.ngrams(words, n)\n",
    "    return FreqDist(ngrams)\n",
    "\n",
    "def get_function_word_frequencies(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    function_words = [word for word in words if word in stop_words]\n",
    "    return FreqDist(function_words)\n",
    "\n",
    "def get_stop_word_frequencies(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words_count = 0\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            stop_words_count += 1\n",
    "    return stop_words_count / len(words)\n",
    "\n",
    "def get_grammar_and_syntax_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    return doc\n",
    "\n",
    "def get_semantic_analysis(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment\n",
    "\n",
    "def get_sentiment_analysis(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)\n",
    "\n",
    "\n",
    "text = read_file('text_file.txt')\n",
    "print('Total number of words:', get_word_count(text))\n",
    "print('Total number of sentences:', get_sentence_count(text))\n",
    "print('Total number of syllables:', get_total_syllable_count(text))\n",
    "print('Average sentence length (in words):', get_avg_sentence_length(text))\n",
    "print('Average word length (in syllables):', get_avg_word_length(text))\n",
    "print('Type-token ratio (TTR):', get_ttr(text))\n",
    "print('Flesch-Kincaid Grade Level:', get_flesch_kincaid_grade_level(text))\n",
    "print('Word length distribution:', get_word_length_distribution(text))\n",
    "print('Sentence length distribution:', get_sentence_length_distribution(text))\n",
    "print('Parts of speech distribution:', get_pos_distribution(text))\n",
    "print('Named entity recognition:', get_named_entity_recognition(text))\n",
    "print('Punctuation distribution:', get_punctuation_distribution(text))\n",
    "print('Capitalization distribution:', get_capitalization_distribution(text))\n",
    "print('Number of unique words in the text:', get_unique_words(text))\n",
    "print('Stem of each word in the text:', get_word_stems(text))\n",
    "print('Average word frequency:', get_word_frequencies(text))\n",
    "print('Collocation frequency:', get_collocation_frequencies(text))\n",
    "print('N-gram frequency:', get_ngram_frequencies(text, 2))\n",
    "print('Function word frequency:', get_function_word_frequencies(text))\n",
    "print('Stop word frequency:', get_stop_word_frequencies(text))\n",
    "print('Grammar and syntax analysis:', get_grammar_and_syntax_analysis(text))\n",
    "print('Semantic analysis:', get_semantic_analysis(text))\n",
    "print('Sentiment analysis:', get_sentiment_analysis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb4184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
